{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source \"data science from scratch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, re\n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "basepath = os.path.dirname('/Users/TheVarunVaria/Downloads/')\n",
    "corpus_root= os.path.abspath(os.path.join(basepath, \"Text\"))\n",
    "wordlists=PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs Sampling\n",
    "#\n",
    "\n",
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # if the total is 7 or less, the first die is equally likely to be\n",
    "        # 1, 2, ..., (total - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # if the total is 7 or more, the first die is equally likely to be\n",
    "        # (total - 6), (total - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2 # doesn't really matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TOPIC MODELING\n",
    "#\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # return the smallest i such that\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "document= []\n",
    "for fileid in wordlists.fileids():\n",
    "    document.append(fileid)\n",
    "print(len(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "\n",
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the k-th topic\"\"\"\n",
    "\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs sampling\n",
      "(3, 5) 38 23\n",
      "(3, 8) 24 32\n",
      "(4, 8) 21 32\n",
      "(1, 4) 21 37\n",
      "(5, 6) 29 34\n",
      "(3, 9) 27 24\n",
      "(1, 2) 25 24\n",
      "(1, 6) 37 29\n",
      "(2, 4) 22 19\n",
      "(4, 6) 30 23\n",
      "(6, 11) 24 29\n",
      "(6, 12) 29 34\n",
      "(2, 6) 35 22\n",
      "(2, 8) 22 26\n",
      "(2, 5) 23 35\n",
      "(6, 7) 25 32\n",
      "(5, 10) 25 26\n",
      "(5, 7) 34 29\n",
      "(3, 4) 17 29\n",
      "(4, 5) 36 23\n",
      "(3, 7) 28 36\n",
      "(4, 9) 30 40\n",
      "(5, 9) 31 27\n",
      "(3, 6) 26 21\n",
      "(2, 7) 36 29\n",
      "(6, 10) 21 22\n",
      "(6, 9) 28 25\n",
      "(4, 7) 30 22\n",
      "(5, 8) 31 32\n",
      "(4, 10) 25 26\n",
      "(6, 8) 34 27\n",
      "(1, 5) 32 29\n",
      "(1, 3) 31 20\n",
      "(1, 7) 26 32\n",
      "(5, 11) 26 26\n",
      "(2, 3) 21 24\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"gibbs sampling\")\n",
    "    comparison = compare_distributions()\n",
    "    for roll, (gibbs, direct) in comparison.items():\n",
    "        print(roll, gibbs, direct)\n",
    "\n",
    "\n",
    "    # topic MODELING\n",
    "\n",
    "    for k, word_counts in enumerate(topic_word_counts):\n",
    "        for word, count in word_counts.most_common():\n",
    "            if count > 0: print(k, word, count)\n",
    "\n",
    "    topic_names = [\"Customer Service\",\n",
    "                   \"Business\",\n",
    "                   \"Stock\",\n",
    "                   \"Management\"]\n",
    "\n",
    "    for document, topic_counts in zip(documents, document_topic_counts):\n",
    "        print(document)\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            if count > 0:\n",
    "                print(topic_names[topic], count)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
